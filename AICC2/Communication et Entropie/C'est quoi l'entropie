Pour une source simple (ie non composée de plusieurs symboles) on peut la voir comme une mesure de l'incertitude
variables aléatoires S = "le soleil se couchera aujourd'hui" vs T = "un lancer de pièce non truquée"
Bah disons qu'il y a très grande chance que le soleil se couche aujourd'hui sauf si la Terre se fait dégommer par un astéroïde - avec proba 0.001.
Du coup il y a presque 0 incertitude -> H(S) = presque 0 bits (ou d'ailleurs 0 n'imp quelle unité, on pourrait utiliser une entropie non binaire)
Par contre le lancer de pile ou face est un modèle de ce qu'il y a de plus incertain comme source binaire. Alors H(T) = 1 bit.

De là, on peut commencer à interpréter l'entropie comme mesure de l'information moyenne apportée: ça ne t'a quasiment rien apporté de savoir que le soleil s'est couché, tu savais que ça allait arriver avec proba 0,999, et la proba que la Terre explose était tellement faible que même si elle apporte bcp d'information (si elle arrive), en moyenne, vu que le Soleil va très sûrement se coucher, tu gagnes presque rien comme info. 
Par contre que la pièce tombe sur pile ou face, tu ne pouvais prédire l'une issue ou l'autre qu'avec une proba 0,5; donc tu ne pouvais carrément pas prédire du tout sauf si t'es du genre à miser sur du 50/50.
Connaître maintenant le résultat de ce pile ou face va donc apporter bcp d'information qu'on tombe sur pile ou sur face (autant en fait car mm proba).

On va prendre un cas intermédiaire: un dé truqué qui tombe sur 3 avec proba 1/2 et dont les autres 5 issues sont équiprobables.
On définit 2 variables aléatoires (aka symboles): 
A dans l'univers (aka l'alphabet) { 3 , not3}
B dans l'alphabet {1,2,3,...6}.
A est équiprob sur ses deux issues, on a toujours H(A) = 1 bit, on l'a définie analogue à un pile (==3) ou face (!=3)
Par contre B n'est pas équiprob sur ses issues. 
Tu vas gagner plus d'information (car il y a plus d'incertitude aka probas plus faibles) à tomber sur CHACUNE DES FACES 1,2,4,5,6 que sur la 3 seule, mais chacune des faces 1,2,4,5,6 n'a pas une si grande proba de tomber. T'as donc une entropie H(B) entre 0 et 1 bit (j'ai pas fait le calcul déso mais pas très important)

Il faut bien garder à l'esprit qu'on mesure l'information gagnée en moyenne, sinon on s'enmêle vite les pinceaux.

Pourquoi: tu connais l'entropie conditionnelle, prenons 2 var aléatoires G et H, H(G|H = x) est la mesure de l'information moyenne qu'apporte G sachant que H = x. Forcément, si G c'est un lancer de pile ou face et H la proba que Conforama ferme (spoilers) aka un truc qui n'a rien à voir, H(G|H = x) = H(G). 
On peut aussi itérer sur toutes les valeurs x que H peut prendre et calculer H(G|H=x) à chaque fois, en les coefficiantant de la proba que H soit = x, on aura H(G|H), mesure de l'information moyenne qu'approte G sachant H.

La première suppose une seule issue de H, la seconde prend en compte toutes les issues. 

Je reviens à mon avertissement sur l'information moyenne, maintenant qu'on a défini l'entropie conditionnelle, je reprends la var aléat S avec le soleil qui se couche avec prob 0,999; astéroïde à 0,001.
J'avais dit que: l'issue astéroïde apporterait bcp d'information, mais vu que sa proba est très faible, finalement l'info moyenne de S est presque nulle (mais pas 0).
Maintenant, si on calculait:
H(S|S = astéroïde)? Ben c'est vrai que savoir que S = astéroïde apporte bcp d'info en soi, genre wow c'est la fin de l'humanité quand mm. Maaaaiiisss on sait que c'est arrivé: on a conditionné sur cet événement. Du coup on a gagné absolument 0 information en moyenne:
H(S| S = astéroïde) = 0, ce qui est encore plus faible que H(S). On s'y attend pas mal: si on te dit qu'une pièce est tombée sur pile, ben tu gagnes rien comme information en moyenne si juste après tu te demandes 'est-ce que c'était pile ou face'

Un dernier ex: on va reprendre nos sources B et A du lancer de dé truqué.
H(B| A = not3)...

maintenant B est distribué uniformément sur les issues 1,2,4,5,6; vu qu'on sait, grâce à A, qu'on n'est PAS tombé sur 3, du coup on a agrandi l'entropie de départ vu qu'il y a + d'incertitude quant à l'issue de B.
Now H(B|A). This time, il faut multiplier par 1/2  H(B|A=not3) et par 1/2 H(B|A=3). Tu peux faire le calcul sans oublier de
coefficienter par les probas, tu verras que H(B|A) < H(B) (*) même si H(B|A=not3) > H(B) (**).
L'inégalité (*) vérifie que conditionner réduit l'entropie, mais la (**) semble crier au bullshit. Faut juste bien garder en tête que ce théorème s'applique à des variables aléatoires entières, PAS à des événements particuliers: B|A=not3, en conditionnant ici on a réduit l'univers de B, on en a tiré une source aléatoire et uniformément distrib sur 5 issues (discrètes). Par contre, B|A, on a observé le résultat de B après avoir observé le résultat de A, et vu que le résultat de A en dit long sur B (il n'y a pas indépendance), on PERD de l'information que B aurait pu donner.
En somme, l'entropie conditionnelle de X sachant Y (et PAS Y = y) est une mesure de l'information additionnelle que X apporte après avoir observé le résultat de Y.

Xav B^Sama je t'encourage à relire les cours et séries depuis la semaine 2 après avoir lu ce gros pavé lol
Avec cette intuition en poche normalement les théorèmes sur le calcul de l'entropie en chaîne et tt le reste ça devrait mieux passer

J'ai juste oublié de dire là haut que H(B|A=3) = 0 forcément 
